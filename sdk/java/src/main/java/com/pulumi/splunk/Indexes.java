// *** WARNING: this file was generated by pulumi-language-java. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.splunk;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Export;
import com.pulumi.core.annotations.ResourceType;
import com.pulumi.core.internal.Codegen;
import com.pulumi.splunk.IndexesArgs;
import com.pulumi.splunk.Utilities;
import com.pulumi.splunk.inputs.IndexesState;
import com.pulumi.splunk.outputs.IndexesAcl;
import java.lang.Boolean;
import java.lang.Integer;
import java.lang.String;
import javax.annotation.Nullable;

/**
 * ## # Resource: splunk.Indexes
 * 
 * Create and manage data indexes.
 * 
 * ## Authorization and authentication
 * 
 * By default, all users can list all indexes. However, if the indexes_list_all capability is enabled in authorize.conf, access to all indexes is limited to only those roles with this capability.
 * To enable indexes_list_all capability restrictions on the data/indexes endpoint, create a [capability::indexes_list_all] stanza in authorize.conf. Specify indexes_list_all=enabled for any role permitted to list all indexes from this endpoint.
 * 
 * ## Example Usage
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.splunk.Indexes;
 * import com.pulumi.splunk.IndexesArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var user01_index = new Indexes("user01-index", IndexesArgs.builder()
 *             .name("user01-index")
 *             .maxHotBuckets(6)
 *             .maxTotalDataSizeMb(1000000)
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 */
@ResourceType(type="splunk:index/indexes:Indexes")
public class Indexes extends com.pulumi.resources.CustomResource {
    /**
     * The app/user context that is the namespace for the resource
     * 
     */
    @Export(name="acl", refs={IndexesAcl.class}, tree="[0]")
    private Output<IndexesAcl> acl;

    /**
     * @return The app/user context that is the namespace for the resource
     * 
     */
    public Output<IndexesAcl> acl() {
        return this.acl;
    }
    /**
     * Controls how many events make up a block for block signatures. If this is set to 0, block signing is disabled for this index. &lt;br&gt;A recommended value is 100.
     * 
     */
    @Export(name="blockSignSize", refs={Integer.class}, tree="[0]")
    private Output<Integer> blockSignSize;

    /**
     * @return Controls how many events make up a block for block signatures. If this is set to 0, block signing is disabled for this index. &lt;br&gt;A recommended value is 100.
     * 
     */
    public Output<Integer> blockSignSize() {
        return this.blockSignSize;
    }
    /**
     * Suggestion for the bucket rebuild process for the size of the time-series (tsidx) file to make.
     * &lt;be&gt;Caution: This is an advanced parameter. Inappropriate use of this parameter causes splunkd to not start if rebuild is required. Do not set this parameter unless instructed by Splunk Support.
     * Default value, auto, varies by the amount of physical RAM on the host&lt;br&gt;
     * less than 2GB RAM = 67108864 (64MB) tsidx
     * 2GB to 8GB RAM = 134217728 (128MB) tsidx
     * more than 8GB RAM = 268435456 (256MB) tsidx&lt;br&gt;
     * Values other than &#34;auto&#34; must be 16MB-1GB. Highest legal value (of the numerical part) is 4294967295 You can specify the value using a size suffix: &#34;16777216&#34; or &#34;16MB&#34; are equivalent.
     * 
     */
    @Export(name="bucketRebuildMemoryHint", refs={String.class}, tree="[0]")
    private Output<String> bucketRebuildMemoryHint;

    /**
     * @return Suggestion for the bucket rebuild process for the size of the time-series (tsidx) file to make.
     * &lt;be&gt;Caution: This is an advanced parameter. Inappropriate use of this parameter causes splunkd to not start if rebuild is required. Do not set this parameter unless instructed by Splunk Support.
     * Default value, auto, varies by the amount of physical RAM on the host&lt;br&gt;
     * less than 2GB RAM = 67108864 (64MB) tsidx
     * 2GB to 8GB RAM = 134217728 (128MB) tsidx
     * more than 8GB RAM = 268435456 (256MB) tsidx&lt;br&gt;
     * Values other than &#34;auto&#34; must be 16MB-1GB. Highest legal value (of the numerical part) is 4294967295 You can specify the value using a size suffix: &#34;16777216&#34; or &#34;16MB&#34; are equivalent.
     * 
     */
    public Output<String> bucketRebuildMemoryHint() {
        return this.bucketRebuildMemoryHint;
    }
    /**
     * An absolute path that contains the colddbs for the index. The path must be readable and writable. Cold databases are opened as needed when searching.
     * 
     */
    @Export(name="coldPath", refs={String.class}, tree="[0]")
    private Output<String> coldPath;

    /**
     * @return An absolute path that contains the colddbs for the index. The path must be readable and writable. Cold databases are opened as needed when searching.
     * 
     */
    public Output<String> coldPath() {
        return this.coldPath;
    }
    /**
     * Destination path for the frozen archive. Use as an alternative to a coldToFrozenScript. Splunk software automatically puts frozen buckets in this directory.
     * &lt;br&gt;
     * Bucket freezing policy is as follows:&lt;br&gt;
     * New style buckets (4.2 and on): removes all files but the rawdata&lt;br&gt;
     * To thaw, run splunk rebuild &lt;bucket dir&gt; on the bucket, then move to the thawed directory&lt;br&gt;
     * Old style buckets (Pre-4.2): gzip all the .data and .tsidx files&lt;br&gt;
     * To thaw, gunzip the zipped files and move the bucket into the thawed directory&lt;br&gt;
     * If both coldToFrozenDir and coldToFrozenScript are specified, coldToFrozenDir takes precedence
     * 
     */
    @Export(name="coldToFrozenDir", refs={String.class}, tree="[0]")
    private Output<String> coldToFrozenDir;

    /**
     * @return Destination path for the frozen archive. Use as an alternative to a coldToFrozenScript. Splunk software automatically puts frozen buckets in this directory.
     * &lt;br&gt;
     * Bucket freezing policy is as follows:&lt;br&gt;
     * New style buckets (4.2 and on): removes all files but the rawdata&lt;br&gt;
     * To thaw, run splunk rebuild &lt;bucket dir&gt; on the bucket, then move to the thawed directory&lt;br&gt;
     * Old style buckets (Pre-4.2): gzip all the .data and .tsidx files&lt;br&gt;
     * To thaw, gunzip the zipped files and move the bucket into the thawed directory&lt;br&gt;
     * If both coldToFrozenDir and coldToFrozenScript are specified, coldToFrozenDir takes precedence
     * 
     */
    public Output<String> coldToFrozenDir() {
        return this.coldToFrozenDir;
    }
    /**
     * Path to the archiving script.
     * &lt;br&gt;If your script requires a program to run it (for example, python), specify the program followed by the path. The script must be in $SPLUNK_HOME/bin or one of its subdirectories.
     * &lt;br&gt;Splunk software ships with an example archiving script in $SPLUNK_HOME/bin called coldToFrozenExample.py. DO NOT use this example script directly. It uses a default path, and if modified in place any changes are overwritten on upgrade.
     * &lt;br&gt;It is best to copy the example script to a new file in bin and modify it for your system. Most importantly, change the default archive path to an existing directory that fits your needs.
     * 
     */
    @Export(name="coldToFrozenScript", refs={String.class}, tree="[0]")
    private Output<String> coldToFrozenScript;

    /**
     * @return Path to the archiving script.
     * &lt;br&gt;If your script requires a program to run it (for example, python), specify the program followed by the path. The script must be in $SPLUNK_HOME/bin or one of its subdirectories.
     * &lt;br&gt;Splunk software ships with an example archiving script in $SPLUNK_HOME/bin called coldToFrozenExample.py. DO NOT use this example script directly. It uses a default path, and if modified in place any changes are overwritten on upgrade.
     * &lt;br&gt;It is best to copy the example script to a new file in bin and modify it for your system. Most importantly, change the default archive path to an existing directory that fits your needs.
     * 
     */
    public Output<String> coldToFrozenScript() {
        return this.coldToFrozenScript;
    }
    /**
     * This parameter is ignored. The splunkd process always compresses raw data.
     * 
     */
    @Export(name="compressRawdata", refs={Boolean.class}, tree="[0]")
    private Output<Boolean> compressRawdata;

    /**
     * @return This parameter is ignored. The splunkd process always compresses raw data.
     * 
     */
    public Output<Boolean> compressRawdata() {
        return this.compressRawdata;
    }
    /**
     * Valid values: (event | metric). Specifies the type of index.
     * 
     */
    @Export(name="datatype", refs={String.class}, tree="[0]")
    private Output<String> datatype;

    /**
     * @return Valid values: (event | metric). Specifies the type of index.
     * 
     */
    public Output<String> datatype() {
        return this.datatype;
    }
    /**
     * Enables asynchronous &#34;online fsck&#34; bucket repair, which runs concurrently with Splunk software.
     * When enabled, you do not have to wait until buckets are repaired to start the Splunk platform. However, you might observe a slight performance degratation.
     * 
     */
    @Export(name="enableOnlineBucketRepair", refs={Boolean.class}, tree="[0]")
    private Output<Boolean> enableOnlineBucketRepair;

    /**
     * @return Enables asynchronous &#34;online fsck&#34; bucket repair, which runs concurrently with Splunk software.
     * When enabled, you do not have to wait until buckets are repaired to start the Splunk platform. However, you might observe a slight performance degratation.
     * 
     */
    public Output<Boolean> enableOnlineBucketRepair() {
        return this.enableOnlineBucketRepair;
    }
    /**
     * Number of seconds after which indexed data rolls to frozen.
     * Defaults to 188697600 (6 years).Freezing data means it is removed from the index. If you need to archive your data, refer to coldToFrozenDir and coldToFrozenScript parameter documentation.
     * 
     */
    @Export(name="frozenTimePeriodInSecs", refs={Integer.class}, tree="[0]")
    private Output<Integer> frozenTimePeriodInSecs;

    /**
     * @return Number of seconds after which indexed data rolls to frozen.
     * Defaults to 188697600 (6 years).Freezing data means it is removed from the index. If you need to archive your data, refer to coldToFrozenDir and coldToFrozenScript parameter documentation.
     * 
     */
    public Output<Integer> frozenTimePeriodInSecs() {
        return this.frozenTimePeriodInSecs;
    }
    /**
     * An absolute path that contains the hot and warm buckets for the index.
     * Required. Splunk software does not start if an index lacks a valid homePath.
     * &lt;br&gt;Caution: The path must be readable and writable.
     * 
     */
    @Export(name="homePath", refs={String.class}, tree="[0]")
    private Output<String> homePath;

    /**
     * @return An absolute path that contains the hot and warm buckets for the index.
     * Required. Splunk software does not start if an index lacks a valid homePath.
     * &lt;br&gt;Caution: The path must be readable and writable.
     * 
     */
    public Output<String> homePath() {
        return this.homePath;
    }
    /**
     * Valid values are: Integer[m|s|h|d].
     * &lt;br&gt;If a warm or cold bucket is older than the specified age, do not create or rebuild its bloomfilter. Specify 0 to never rebuild bloomfilters.
     * 
     */
    @Export(name="maxBloomBackfillBucketAge", refs={String.class}, tree="[0]")
    private Output<String> maxBloomBackfillBucketAge;

    /**
     * @return Valid values are: Integer[m|s|h|d].
     * &lt;br&gt;If a warm or cold bucket is older than the specified age, do not create or rebuild its bloomfilter. Specify 0 to never rebuild bloomfilters.
     * 
     */
    public Output<String> maxBloomBackfillBucketAge() {
        return this.maxBloomBackfillBucketAge;
    }
    /**
     * The number of concurrent optimize processes that can run against a hot bucket.
     * This number should be increased if instructed by Splunk Support. Typically the default value should suffice.
     * 
     */
    @Export(name="maxConcurrentOptimizes", refs={Integer.class}, tree="[0]")
    private Output<Integer> maxConcurrentOptimizes;

    /**
     * @return The number of concurrent optimize processes that can run against a hot bucket.
     * This number should be increased if instructed by Splunk Support. Typically the default value should suffice.
     * 
     */
    public Output<Integer> maxConcurrentOptimizes() {
        return this.maxConcurrentOptimizes;
    }
    /**
     * The maximum size in MB for a hot DB to reach before a roll to warm is triggered. Specifying &#34;auto&#34; or &#34;auto_high_volume&#34; causes Splunk software to autotune this parameter (recommended).
     * Use &#34;auto_high_volume&#34; for high volume indexes (such as the main index); otherwise, use &#34;auto&#34;. A &#34;high volume index&#34; would typically be considered one that gets over 10GB of data per day.
     * 
     */
    @Export(name="maxDataSize", refs={String.class}, tree="[0]")
    private Output<String> maxDataSize;

    /**
     * @return The maximum size in MB for a hot DB to reach before a roll to warm is triggered. Specifying &#34;auto&#34; or &#34;auto_high_volume&#34; causes Splunk software to autotune this parameter (recommended).
     * Use &#34;auto_high_volume&#34; for high volume indexes (such as the main index); otherwise, use &#34;auto&#34;. A &#34;high volume index&#34; would typically be considered one that gets over 10GB of data per day.
     * 
     */
    public Output<String> maxDataSize() {
        return this.maxDataSize;
    }
    /**
     * Maximum hot buckets that can exist per index. Defaults to 3.
     * &lt;br&gt;When maxHotBuckets is exceeded, Splunk software rolls the least recently used (LRU) hot bucket to warm. Both normal hot buckets and quarantined hot buckets count towards this total. This setting operates independently of maxHotIdleSecs, which can also cause hot buckets to roll.
     * 
     */
    @Export(name="maxHotBuckets", refs={Integer.class}, tree="[0]")
    private Output<Integer> maxHotBuckets;

    /**
     * @return Maximum hot buckets that can exist per index. Defaults to 3.
     * &lt;br&gt;When maxHotBuckets is exceeded, Splunk software rolls the least recently used (LRU) hot bucket to warm. Both normal hot buckets and quarantined hot buckets count towards this total. This setting operates independently of maxHotIdleSecs, which can also cause hot buckets to roll.
     * 
     */
    public Output<Integer> maxHotBuckets() {
        return this.maxHotBuckets;
    }
    /**
     * Maximum life, in seconds, of a hot bucket. Defaults to 0. If a hot bucket exceeds maxHotIdleSecs, Splunk software rolls it to warm. This setting operates independently of maxHotBuckets, which can also cause hot buckets to roll. A value of 0 turns off the idle check (equivalent to INFINITE idle time).
     * 
     */
    @Export(name="maxHotIdleSecs", refs={Integer.class}, tree="[0]")
    private Output<Integer> maxHotIdleSecs;

    /**
     * @return Maximum life, in seconds, of a hot bucket. Defaults to 0. If a hot bucket exceeds maxHotIdleSecs, Splunk software rolls it to warm. This setting operates independently of maxHotBuckets, which can also cause hot buckets to roll. A value of 0 turns off the idle check (equivalent to INFINITE idle time).
     * 
     */
    public Output<Integer> maxHotIdleSecs() {
        return this.maxHotIdleSecs;
    }
    /**
     * Upper bound of target maximum timespan of hot/warm buckets in seconds. Defaults to 7776000 seconds (90 days).
     * 
     */
    @Export(name="maxHotSpanSecs", refs={Integer.class}, tree="[0]")
    private Output<Integer> maxHotSpanSecs;

    /**
     * @return Upper bound of target maximum timespan of hot/warm buckets in seconds. Defaults to 7776000 seconds (90 days).
     * 
     */
    public Output<Integer> maxHotSpanSecs() {
        return this.maxHotSpanSecs;
    }
    /**
     * The amount of memory, expressed in MB, to allocate for buffering a single tsidx file into memory before flushing to disk. Defaults to 5. The default is recommended for all environments.
     * 
     */
    @Export(name="maxMemMb", refs={Integer.class}, tree="[0]")
    private Output<Integer> maxMemMb;

    /**
     * @return The amount of memory, expressed in MB, to allocate for buffering a single tsidx file into memory before flushing to disk. Defaults to 5. The default is recommended for all environments.
     * 
     */
    public Output<Integer> maxMemMb() {
        return this.maxMemMb;
    }
    /**
     * Upper limit, in seconds, on how long an event can sit in raw slice. Applies only if replication is enabled for this index. Otherwise ignored. If there are any acknowledged events sharing this raw slice, this paramater does not apply. In this case, maxTimeUnreplicatedWithAcks applies. Highest legal value is 2147483647. To disable this parameter, set to 0.
     * 
     */
    @Export(name="maxMetaEntries", refs={Integer.class}, tree="[0]")
    private Output<Integer> maxMetaEntries;

    /**
     * @return Upper limit, in seconds, on how long an event can sit in raw slice. Applies only if replication is enabled for this index. Otherwise ignored. If there are any acknowledged events sharing this raw slice, this paramater does not apply. In this case, maxTimeUnreplicatedWithAcks applies. Highest legal value is 2147483647. To disable this parameter, set to 0.
     * 
     */
    public Output<Integer> maxMetaEntries() {
        return this.maxMetaEntries;
    }
    /**
     * Upper limit, in seconds, on how long an event can sit in raw slice. Applies only if replication is enabled for this index. Otherwise ignored.
     * If there are any acknowledged events sharing this raw slice, this paramater does not apply. In this case, maxTimeUnreplicatedWithAcks applies.
     * Highest legal value is 2147483647. To disable this parameter, set to 0.
     * 
     */
    @Export(name="maxTimeUnreplicatedNoAcks", refs={Integer.class}, tree="[0]")
    private Output<Integer> maxTimeUnreplicatedNoAcks;

    /**
     * @return Upper limit, in seconds, on how long an event can sit in raw slice. Applies only if replication is enabled for this index. Otherwise ignored.
     * If there are any acknowledged events sharing this raw slice, this paramater does not apply. In this case, maxTimeUnreplicatedWithAcks applies.
     * Highest legal value is 2147483647. To disable this parameter, set to 0.
     * 
     */
    public Output<Integer> maxTimeUnreplicatedNoAcks() {
        return this.maxTimeUnreplicatedNoAcks;
    }
    /**
     * Upper limit, in seconds, on how long events can sit unacknowledged in a raw slice. Applies only if you have enabled acks on forwarders and have replication enabled (with clustering).
     * Note: This is an advanced parameter. Make sure you understand the settings on all forwarders before changing this. This number should not exceed ack timeout configured on any forwarder, and should actually be set to at most half of the minimum value of that timeout. You can find this setting in outputs.conf readTimeout setting under the tcpout stanza.
     * To disable, set to 0, but this is NOT recommended. Highest legal value is 2147483647.
     * 
     */
    @Export(name="maxTimeUnreplicatedWithAcks", refs={Integer.class}, tree="[0]")
    private Output<Integer> maxTimeUnreplicatedWithAcks;

    /**
     * @return Upper limit, in seconds, on how long events can sit unacknowledged in a raw slice. Applies only if you have enabled acks on forwarders and have replication enabled (with clustering).
     * Note: This is an advanced parameter. Make sure you understand the settings on all forwarders before changing this. This number should not exceed ack timeout configured on any forwarder, and should actually be set to at most half of the minimum value of that timeout. You can find this setting in outputs.conf readTimeout setting under the tcpout stanza.
     * To disable, set to 0, but this is NOT recommended. Highest legal value is 2147483647.
     * 
     */
    public Output<Integer> maxTimeUnreplicatedWithAcks() {
        return this.maxTimeUnreplicatedWithAcks;
    }
    /**
     * The maximum size of an index (in MB). If an index grows larger than the maximum size, the oldest data is frozen.
     * 
     */
    @Export(name="maxTotalDataSizeMb", refs={Integer.class}, tree="[0]")
    private Output<Integer> maxTotalDataSizeMb;

    /**
     * @return The maximum size of an index (in MB). If an index grows larger than the maximum size, the oldest data is frozen.
     * 
     */
    public Output<Integer> maxTotalDataSizeMb() {
        return this.maxTotalDataSizeMb;
    }
    /**
     * The maximum number of warm buckets. If this number is exceeded, the warm bucket/s with the lowest value for their latest times is moved to cold.
     * 
     */
    @Export(name="maxWarmDbCount", refs={Integer.class}, tree="[0]")
    private Output<Integer> maxWarmDbCount;

    /**
     * @return The maximum number of warm buckets. If this number is exceeded, the warm bucket/s with the lowest value for their latest times is moved to cold.
     * 
     */
    public Output<Integer> maxWarmDbCount() {
        return this.maxWarmDbCount;
    }
    /**
     * Specify an integer (or &#34;disable&#34;) for this parameter.
     * This parameter sets how frequently splunkd forces a filesystem sync while compressing journal slices.
     * During this period, uncompressed slices are left on disk even after they are compressed. Then splunkd forces a filesystem sync of the compressed journal and removes the accumulated uncompressed files.
     * If 0 is specified, splunkd forces a filesystem sync after every slice completes compressing. Specifying &#34;disable&#34; disables syncing entirely: uncompressed slices are removed as soon as compression is complete.
     * 
     */
    @Export(name="minRawFileSyncSecs", refs={String.class}, tree="[0]")
    private Output<String> minRawFileSyncSecs;

    /**
     * @return Specify an integer (or &#34;disable&#34;) for this parameter.
     * This parameter sets how frequently splunkd forces a filesystem sync while compressing journal slices.
     * During this period, uncompressed slices are left on disk even after they are compressed. Then splunkd forces a filesystem sync of the compressed journal and removes the accumulated uncompressed files.
     * If 0 is specified, splunkd forces a filesystem sync after every slice completes compressing. Specifying &#34;disable&#34; disables syncing entirely: uncompressed slices are removed as soon as compression is complete.
     * 
     */
    public Output<String> minRawFileSyncSecs() {
        return this.minRawFileSyncSecs;
    }
    /**
     * Minimum size of the queue that stores events in memory before committing them to a tsidx file.
     * 
     */
    @Export(name="minStreamGroupQueueSize", refs={Integer.class}, tree="[0]")
    private Output<Integer> minStreamGroupQueueSize;

    /**
     * @return Minimum size of the queue that stores events in memory before committing them to a tsidx file.
     * 
     */
    public Output<Integer> minStreamGroupQueueSize() {
        return this.minStreamGroupQueueSize;
    }
    /**
     * The name of the index to create.
     * 
     */
    @Export(name="name", refs={String.class}, tree="[0]")
    private Output<String> name;

    /**
     * @return The name of the index to create.
     * 
     */
    public Output<String> name() {
        return this.name;
    }
    /**
     * Related to serviceMetaPeriod. If set, it enables metadata sync every &lt;integer&gt; seconds, but only for records where the sync can be done efficiently in-place, without requiring a full re-write of the metadata file. Records that require full re-write are be sync&#39;ed at serviceMetaPeriod.
     * partialServiceMetaPeriod specifies, in seconds, how frequently it should sync. Zero means that this feature is turned off and serviceMetaPeriod is the only time when metadata sync happens.
     * If the value of partialServiceMetaPeriod is greater than serviceMetaPeriod, this setting has no effect.
     * By default it is turned off (zero).
     * 
     */
    @Export(name="partialServiceMetaPeriod", refs={Integer.class}, tree="[0]")
    private Output<Integer> partialServiceMetaPeriod;

    /**
     * @return Related to serviceMetaPeriod. If set, it enables metadata sync every &lt;integer&gt; seconds, but only for records where the sync can be done efficiently in-place, without requiring a full re-write of the metadata file. Records that require full re-write are be sync&#39;ed at serviceMetaPeriod.
     * partialServiceMetaPeriod specifies, in seconds, how frequently it should sync. Zero means that this feature is turned off and serviceMetaPeriod is the only time when metadata sync happens.
     * If the value of partialServiceMetaPeriod is greater than serviceMetaPeriod, this setting has no effect.
     * By default it is turned off (zero).
     * 
     */
    public Output<Integer> partialServiceMetaPeriod() {
        return this.partialServiceMetaPeriod;
    }
    /**
     * Specifies, in seconds, how often the indexer checks the status of the child OS processes it launched to see if it can launch new processes for queued requests. Defaults to 15.
     * If set to 0, the indexer checks child process status every second.
     * Highest legal value is 4294967295.
     * 
     */
    @Export(name="processTrackerServiceInterval", refs={Integer.class}, tree="[0]")
    private Output<Integer> processTrackerServiceInterval;

    /**
     * @return Specifies, in seconds, how often the indexer checks the status of the child OS processes it launched to see if it can launch new processes for queued requests. Defaults to 15.
     * If set to 0, the indexer checks child process status every second.
     * Highest legal value is 4294967295.
     * 
     */
    public Output<Integer> processTrackerServiceInterval() {
        return this.processTrackerServiceInterval;
    }
    /**
     * Events with timestamp of quarantineFutureSecs newer than &#34;now&#34; are dropped into quarantine bucket. Defaults to 2592000 (30 days).
     * This is a mechanism to prevent main hot buckets from being polluted with fringe events.
     * 
     */
    @Export(name="quarantineFutureSecs", refs={Integer.class}, tree="[0]")
    private Output<Integer> quarantineFutureSecs;

    /**
     * @return Events with timestamp of quarantineFutureSecs newer than &#34;now&#34; are dropped into quarantine bucket. Defaults to 2592000 (30 days).
     * This is a mechanism to prevent main hot buckets from being polluted with fringe events.
     * 
     */
    public Output<Integer> quarantineFutureSecs() {
        return this.quarantineFutureSecs;
    }
    /**
     * Events with timestamp of quarantinePastSecs older than &#34;now&#34; are dropped into quarantine bucket. Defaults to 77760000 (900 days). This is a mechanism to prevent the main hot buckets from being polluted with fringe events.
     * 
     */
    @Export(name="quarantinePastSecs", refs={Integer.class}, tree="[0]")
    private Output<Integer> quarantinePastSecs;

    /**
     * @return Events with timestamp of quarantinePastSecs older than &#34;now&#34; are dropped into quarantine bucket. Defaults to 77760000 (900 days). This is a mechanism to prevent the main hot buckets from being polluted with fringe events.
     * 
     */
    public Output<Integer> quarantinePastSecs() {
        return this.quarantinePastSecs;
    }
    /**
     * Target uncompressed size in bytes for individual raw slice in the rawdata journal of the index. Defaults to 131072 (128KB). 0 is not a valid value. If 0 is specified, rawChunkSizeBytes is set to the default value.
     * 
     */
    @Export(name="rawChunkSizeBytes", refs={Integer.class}, tree="[0]")
    private Output<Integer> rawChunkSizeBytes;

    /**
     * @return Target uncompressed size in bytes for individual raw slice in the rawdata journal of the index. Defaults to 131072 (128KB). 0 is not a valid value. If 0 is specified, rawChunkSizeBytes is set to the default value.
     * 
     */
    public Output<Integer> rawChunkSizeBytes() {
        return this.rawChunkSizeBytes;
    }
    /**
     * Index replication control. This parameter applies to only clustering slaves.
     * auto = Use the master index replication configuration value.
     * 0 = Turn off replication for this index.
     * 
     */
    @Export(name="repFactor", refs={String.class}, tree="[0]")
    private Output<String> repFactor;

    /**
     * @return Index replication control. This parameter applies to only clustering slaves.
     * auto = Use the master index replication configuration value.
     * 0 = Turn off replication for this index.
     * 
     */
    public Output<String> repFactor() {
        return this.repFactor;
    }
    /**
     * How frequently (in seconds) to check if a new hot bucket needs to be created. Also, how frequently to check if there are any warm/cold buckets that should be rolled/frozen.
     * 
     */
    @Export(name="rotatePeriodInSecs", refs={Integer.class}, tree="[0]")
    private Output<Integer> rotatePeriodInSecs;

    /**
     * @return How frequently (in seconds) to check if a new hot bucket needs to be created. Also, how frequently to check if there are any warm/cold buckets that should be rolled/frozen.
     * 
     */
    public Output<Integer> rotatePeriodInSecs() {
        return this.rotatePeriodInSecs;
    }
    /**
     * Defines how frequently metadata is synced to disk, in seconds. Defaults to 25 (seconds).
     * You may want to set this to a higher value if the sum of your metadata file sizes is larger than many tens of megabytes, to avoid the hit on I/O in the indexing fast path.
     * 
     */
    @Export(name="serviceMetaPeriod", refs={Integer.class}, tree="[0]")
    private Output<Integer> serviceMetaPeriod;

    /**
     * @return Defines how frequently metadata is synced to disk, in seconds. Defaults to 25 (seconds).
     * You may want to set this to a higher value if the sum of your metadata file sizes is larger than many tens of megabytes, to avoid the hit on I/O in the indexing fast path.
     * 
     */
    public Output<Integer> serviceMetaPeriod() {
        return this.serviceMetaPeriod;
    }
    /**
     * When true, a sync operation is called before file descriptor is closed on metadata file updates. This functionality improves integrity of metadata files, especially in regards to operating system crashes/machine failures.
     * 
     */
    @Export(name="syncMeta", refs={Boolean.class}, tree="[0]")
    private Output<Boolean> syncMeta;

    /**
     * @return When true, a sync operation is called before file descriptor is closed on metadata file updates. This functionality improves integrity of metadata files, especially in regards to operating system crashes/machine failures.
     * 
     */
    public Output<Boolean> syncMeta() {
        return this.syncMeta;
    }
    /**
     * An absolute path that contains the thawed (resurrected) databases for the index.
     * Cannot be defined in terms of a volume definition.
     * Required. Splunk software does not start if an index lacks a valid thawedPath.
     * 
     */
    @Export(name="thawedPath", refs={String.class}, tree="[0]")
    private Output<String> thawedPath;

    /**
     * @return An absolute path that contains the thawed (resurrected) databases for the index.
     * Cannot be defined in terms of a volume definition.
     * Required. Splunk software does not start if an index lacks a valid thawedPath.
     * 
     */
    public Output<String> thawedPath() {
        return this.thawedPath;
    }
    /**
     * Defines how frequently Splunk software checks for index throttling condition, in seconds. Defaults to 15 (seconds).
     * 
     */
    @Export(name="throttleCheckPeriod", refs={Integer.class}, tree="[0]")
    private Output<Integer> throttleCheckPeriod;

    /**
     * @return Defines how frequently Splunk software checks for index throttling condition, in seconds. Defaults to 15 (seconds).
     * 
     */
    public Output<Integer> throttleCheckPeriod() {
        return this.throttleCheckPeriod;
    }
    /**
     * Location to store datamodel acceleration TSIDX data for this index. Restart splunkd after changing this parameter.
     * If specified, it must be defined in terms of a volume definition.
     * 
     */
    @Export(name="tstatsHomePath", refs={String.class}, tree="[0]")
    private Output<String> tstatsHomePath;

    /**
     * @return Location to store datamodel acceleration TSIDX data for this index. Restart splunkd after changing this parameter.
     * If specified, it must be defined in terms of a volume definition.
     * 
     */
    public Output<String> tstatsHomePath() {
        return this.tstatsHomePath;
    }
    /**
     * Path to a script to run when moving data from warm to cold.
     * This attribute is supported for backwards compatibility with Splunk software versions older than 4.0. Contact Splunk support if you need help configuring this setting.
     * 
     */
    @Export(name="warmToColdScript", refs={String.class}, tree="[0]")
    private Output<String> warmToColdScript;

    /**
     * @return Path to a script to run when moving data from warm to cold.
     * This attribute is supported for backwards compatibility with Splunk software versions older than 4.0. Contact Splunk support if you need help configuring this setting.
     * 
     */
    public Output<String> warmToColdScript() {
        return this.warmToColdScript;
    }

    /**
     *
     * @param name The _unique_ name of the resulting resource.
     */
    public Indexes(java.lang.String name) {
        this(name, IndexesArgs.Empty);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     */
    public Indexes(java.lang.String name, @Nullable IndexesArgs args) {
        this(name, args, null);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param options A bag of options that control this resource's behavior.
     */
    public Indexes(java.lang.String name, @Nullable IndexesArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("splunk:index/indexes:Indexes", name, makeArgs(args, options), makeResourceOptions(options, Codegen.empty()), false);
    }

    private Indexes(java.lang.String name, Output<java.lang.String> id, @Nullable IndexesState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("splunk:index/indexes:Indexes", name, state, makeResourceOptions(options, id), false);
    }

    private static IndexesArgs makeArgs(@Nullable IndexesArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        if (options != null && options.getUrn().isPresent()) {
            return null;
        }
        return args == null ? IndexesArgs.Empty : args;
    }

    private static com.pulumi.resources.CustomResourceOptions makeResourceOptions(@Nullable com.pulumi.resources.CustomResourceOptions options, @Nullable Output<java.lang.String> id) {
        var defaultOptions = com.pulumi.resources.CustomResourceOptions.builder()
            .version(Utilities.getVersion())
            .build();
        return com.pulumi.resources.CustomResourceOptions.merge(defaultOptions, options, id);
    }

    /**
     * Get an existing Host resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state
     * @param options Optional settings to control the behavior of the CustomResource.
     */
    public static Indexes get(java.lang.String name, Output<java.lang.String> id, @Nullable IndexesState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        return new Indexes(name, id, state, options);
    }
}
